{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iYWqyU4xI8Sq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.init\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.utils.data as torchdata\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import MNIST,CIFAR10\n",
        "from torchvision.transforms import Compose, Lambda,ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from random import shuffle\n",
        "\n",
        "import json\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0my_ufdisEj"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-yFFPz7YinRR"
      },
      "outputs": [],
      "source": [
        "def class_accuracy(pred: torch.Tensor, true: torch.Tensor) -> float:\n",
        "    pred = pred.to('cuda')\n",
        "    true = true.to('cuda')\n",
        "    return 100 * (pred.int() == true.int()).sum().item() / len(true)\n",
        "\n",
        "\n",
        "def kl_divergence(z_posterior_means, z_posterior_log_std, z_prior_mean=0.0, z_prior_log_std=0.0):\n",
        "    z_prior_means = torch.full_like(z_posterior_means, z_prior_mean)\n",
        "    z_prior_log_stds = torch.full_like(z_posterior_log_std, z_prior_log_std)\n",
        "\n",
        "    prior_precision = torch.exp(torch.mul(z_prior_log_stds, -2))\n",
        "    kl = 0.5 * ((z_posterior_means - z_prior_means) ** 2) * prior_precision - 0.5\n",
        "    kl += z_prior_log_stds - z_posterior_log_std\n",
        "    kl += 0.5 * torch.exp(2 * z_posterior_log_std - 2 * z_prior_log_stds)\n",
        "    return torch.sum(kl, dim=(1,))\n",
        "\n",
        "\n",
        "def bernoulli_log_likelihood(x_observed, x_reconstructed, epsilon=1e-8) -> torch.Tensor:\n",
        "    prob = torch.mul(torch.log(x_reconstructed + epsilon), x_observed)\n",
        "    inv_prob = torch.mul(torch.log(1 - x_reconstructed + epsilon), 1 - x_observed)\n",
        "    inv_prob[inv_prob != inv_prob] = epsilon\n",
        "\n",
        "    return torch.sum(torch.add(prob, inv_prob), dim=(1,))\n",
        "\n",
        "\n",
        "def normal_with_reparameterization(means: torch.Tensor, log_stds: torch.Tensor, device='cuda') -> torch.Tensor:\n",
        "    return torch.add(means, torch.mul(torch.exp(log_stds), torch.randn_like(means)))\n",
        "\n",
        "\n",
        "def concatenate_flattened(tensor_list) -> torch.Tensor:\n",
        "    return torch.cat([torch.reshape(t, (-1,)) for t in tensor_list])\n",
        "\n",
        "\n",
        "def task_subset(data: torchdata.Dataset, task_ids: torch.Tensor, task: int, ) -> torch.Tensor:\n",
        "    idx_list = torch.arange(0, len(task_ids))[task_ids == task]\n",
        "    return torchdata.Subset(data, idx_list)\n",
        "\n",
        "class Flatten(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return np.array(sample, dtype=np.float32).flatten()\n",
        "\n",
        "\n",
        "class Scale(object):\n",
        "    def __init__(self, max_value=255):\n",
        "        self.max_value = max_value\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return sample / self.max_value\n",
        "\n",
        "\n",
        "class Permute(object):\n",
        "    def __init__(self, permutation):\n",
        "        self.permutation = permutation\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return sample[self.permutation]\n",
        "\n",
        "\n",
        "OUT_DIR = 'out/experiments/'\n",
        "MODEL_DIR = 'out/models/'\n",
        "IMAGE_DIR = 'out/images/'\n",
        "\n",
        "\n",
        "def write_as_json(filename, data):\n",
        "    if not os.path.exists(os.path.dirname(OUT_DIR + filename)):\n",
        "        print('creating ...')\n",
        "        os.makedirs(os.path.dirname(OUT_DIR + filename))\n",
        "\n",
        "    with open(OUT_DIR + filename, \"w\") as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "\n",
        "def save_model(model, filename):\n",
        "    if not os.path.exists(os.path.dirname(MODEL_DIR)):\n",
        "        print('creating ...')\n",
        "        os.makedirs(os.path.dirname(MODEL_DIR))\n",
        "\n",
        "    torch.save(model, MODEL_DIR + filename)\n",
        "\n",
        "\n",
        "def load_model(filename):\n",
        "    if not os.path.exists(os.path.dirname(MODEL_DIR)):\n",
        "        raise FileNotFoundError()\n",
        "    return torch.load(MODEL_DIR + filename)\n",
        "\n",
        "\n",
        "def save_generated_image(data: np.ndarray, filename: str):\n",
        "    if not os.path.exists(os.path.dirname(IMAGE_DIR)):\n",
        "        print('creating ...')\n",
        "        os.makedirs(os.path.dirname(IMAGE_DIR))\n",
        "\n",
        "    data = data * 255\n",
        "    image = Image.fromarray(data)\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    image.save(IMAGE_DIR + filename)\n",
        "    np.save(IMAGE_DIR + filename + str('.npy'), data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZCqPEOdOjsp"
      },
      "source": [
        "# Experiment Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hkF7CyFmOloI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_point_estimate_initialisation(model, data, epochs, task_ids, batch_size,\n",
        "                                      device, lr, task_idx=0, y_transform=None,\n",
        "                                      multiheaded=True):\n",
        "\n",
        "    print(\"Obtaining point estimate for posterior initialisation\")\n",
        "\n",
        "    head = task_idx if multiheaded else 0\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    task_data = task_subset(data, task_ids, task_idx)\n",
        "    loader = DataLoader(task_data, batch_size)\n",
        "\n",
        "    for _ in tqdm(range(epochs), 'Epochs: '):\n",
        "        for batch in loader:\n",
        "            optimizer.zero_grad()\n",
        "            x, y_true = batch\n",
        "            x = x.to(device)\n",
        "            y_true = y_true.to(device)\n",
        "\n",
        "            if y_transform is not None:\n",
        "                y_true = y_transform(y_true, task_idx)\n",
        "\n",
        "            loss = model.point_estimate_loss(x, y_true, head_idx=head)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "def run_task(model, task_idx,\n",
        "             train_data, train_task_ids, test_data, test_task_ids,\n",
        "             coreset, epochs, batch_size, save_as, device, lr,num_tasks,\n",
        "             y_transform=None, multiheaded=True, train_full_coreset=True,\n",
        "             summary_writer=None):\n",
        "\n",
        "\n",
        "    print('TASK ', task_idx)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    head = task_idx if multiheaded else 0\n",
        "\n",
        "    task_data = task_subset(train_data, train_task_ids, task_idx)\n",
        "    non_coreset_data = coreset.select(task_data, task_id=task_idx)\n",
        "    train_loader = DataLoader(non_coreset_data, batch_size)\n",
        "\n",
        "    for epoch in tqdm(range(epochs), 'Epochs: '):\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            x, y_true = batch\n",
        "            x = x.to(device)\n",
        "            y_true = y_true.to(device)\n",
        "\n",
        "            if y_transform is not None:\n",
        "                y_true = y_transform(y_true, task_idx)\n",
        "\n",
        "            loss = model.vcl_loss(x, y_true, head, len(task_data))\n",
        "            epoch_loss += len(x) * loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if summary_writer is not None:\n",
        "            summary_writer.add_scalars(\"loss\", {\"TASK_\" + str(task_idx): epoch_loss / len(task_data)}, epoch)\n",
        "\n",
        "    model.reset_for_new_task(head)\n",
        "\n",
        "    if train_full_coreset:\n",
        "        model_cs_trained = coreset.coreset_train(\n",
        "            model, optimizer, list(range(task_idx + 1)), epochs,\n",
        "            device, y_transform=y_transform, multiheaded=multiheaded)\n",
        "\n",
        "    np_task_accuracies = np.full(num_tasks, np.nan)\n",
        "    task_accuracies = [-1]*num_tasks\n",
        "    tot_right = 0\n",
        "    tot_tested = 0\n",
        "    for test_task_idx in range(task_idx+1):\n",
        "        if not train_full_coreset:\n",
        "            model_cs_trained = coreset.coreset_train(\n",
        "                model, optimizer, test_task_idx, epochs,\n",
        "                device, y_transform=y_transform, multiheaded=multiheaded)\n",
        "\n",
        "        head = test_task_idx if multiheaded else 0\n",
        "\n",
        "        task_data = task_subset(test_data, test_task_ids, test_task_idx)\n",
        "\n",
        "        x = torch.Tensor(np.array([x for x, _ in task_data]))\n",
        "        y_true = torch.Tensor([y for _, y in task_data])\n",
        "        x = x.to(device)\n",
        "\n",
        "        if y_transform is not None:\n",
        "            y_true = y_transform(y_true, test_task_idx)\n",
        "\n",
        "        y_pred = model_cs_trained.prediction(x, head)\n",
        "\n",
        "        acc = class_accuracy(y_pred, y_true)\n",
        "        print(\"After task {} perfomance on task {} is {}\"\n",
        "              .format(task_idx, test_task_idx, acc))\n",
        "        tot_right += acc * len(task_data)\n",
        "        tot_tested += len(task_data)\n",
        "        np_task_accuracies[test_task_idx] = acc\n",
        "        task_accuracies[test_task_idx] = acc\n",
        "\n",
        "    mean_accuracy = tot_right / tot_tested\n",
        "    print(\"Mean accuracy:\", mean_accuracy)\n",
        "\n",
        "    if summary_writer is not None:\n",
        "        task_accuracies_dict = dict(zip([\"TASK_\" + str(i) for i in range(task_idx + 1)], task_accuracies))\n",
        "        summary_writer.add_scalars(\"test_accuracy\", task_accuracies_dict, task_idx + 1)\n",
        "        summary_writer.add_scalar(\"mean_accuracy\", mean_accuracy, task_idx + 1)\n",
        "\n",
        "    write_as_json(save_as + '/accuracy.txt', task_accuracies)\n",
        "    save_model(model, save_as + '_model_task_' + str(task_idx) + '.pth')\n",
        "    return np_task_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73vKAaqYOYYd"
      },
      "source": [
        "# Coreset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "E6VZd9HC2Pb9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class Coreset:\n",
        "    def __init__(self, size=0, lr=0.001):\n",
        "        self.size = size\n",
        "        self.coreset = None\n",
        "        self.coreset_task_ids = None\n",
        "        self.lr = lr\n",
        "\n",
        "    def select(self, d: torchdata.Dataset, task_id: int):\n",
        "        return d\n",
        "\n",
        "    def coreset_train(self, m, old_optimizer, tasks, epochs, device,\n",
        "                      y_transform=None, multiheaded=True, batch_size=256):\n",
        "\n",
        "        if self.coreset is None:\n",
        "            return m\n",
        "\n",
        "        model = deepcopy(m)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
        "        optimizer.load_state_dict(old_optimizer.state_dict())\n",
        "\n",
        "        if isinstance(tasks, int):\n",
        "            tasks = [tasks]\n",
        "\n",
        "        train_loaders = {\n",
        "            task_idx: torchdata.DataLoader(\n",
        "                task_subset(self.coreset, self.coreset_task_ids, task_idx),\n",
        "                batch_size\n",
        "            )\n",
        "            for task_idx in tasks\n",
        "        }\n",
        "\n",
        "        print('CORESET TRAIN')\n",
        "        for _ in tqdm(range(epochs), 'Epochs: '):\n",
        "            shuffle(tasks)\n",
        "            for task_idx in tasks:\n",
        "                head = task_idx if multiheaded else 0\n",
        "\n",
        "                for batch in train_loaders[task_idx]:\n",
        "                    optimizer.zero_grad()\n",
        "                    x, y_true = batch\n",
        "                    x = x.to(device)\n",
        "                    y_true = y_true.to(device)\n",
        "\n",
        "                    if y_transform is not None:\n",
        "                        y_true = y_transform(y_true, task_idx)\n",
        "\n",
        "                    loss = model.vcl_loss(x, y_true, head, len(self.coreset))\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "        return model\n",
        "\n",
        "class RandomCoreset(Coreset):\n",
        "    def __init__(self, size):\n",
        "        super().__init__(size)\n",
        "\n",
        "    def select(self, d: torchdata.Dataset, task_id: int):\n",
        "\n",
        "        new_cs_data, non_cs = torchdata.random_split(d, [self.size, max(0, len(d) - self.size)])\n",
        "\n",
        "\n",
        "        new_cs_x = torch.tensor(np.array([x for x, _ in new_cs_data]))\n",
        "        new_cs_y = torch.tensor(np.array([y for _, y in new_cs_data]))\n",
        "\n",
        "        new_cs = torchdata.TensorDataset(new_cs_x, new_cs_y)\n",
        "        new_task_ids = torch.full((len(new_cs_data),), task_id)\n",
        "\n",
        "        if self.coreset is None:\n",
        "            self.coreset = new_cs\n",
        "            self.coreset_task_ids = new_task_ids\n",
        "        else:\n",
        "            self.coreset = torchdata.ConcatDataset((self.coreset, new_cs))\n",
        "            self.coreset_task_ids = torch.cat((self.coreset_task_ids, new_task_ids))\n",
        "\n",
        "        return non_cs\n",
        "class kCenterCoreset(Coreset):\n",
        "  def __init__(self,size):\n",
        "    super().__init__(size)\n",
        "  def select(self,d, task_id):\n",
        "    x,y = zip(*[(data[0],data[1]) for data in d])\n",
        "    x = torch.Tensor(np.array(x).flatten())\n",
        "    y = torch.Tensor(np.array(y))\n",
        "    dists = torch.full((len(x),), float('inf'))\n",
        "    current_id = 0\n",
        "    dists = self.update_distance(dists, x, current_id)\n",
        "    idx = [current_id]\n",
        "    for _ in range(1, self.size):\n",
        "        current_id = torch.argmax(dists)\n",
        "        dists = self.update_distance(dists, x, current_id)\n",
        "        idx.append(current_id.item())\n",
        "    idx = torch.tensor(idx)\n",
        "\n",
        "    new_cs_x = x[idx]\n",
        "    new_cs_y = y[idx]\n",
        "    new_non_cs_x = torch.stack([x for i, x in enumerate(x) if i not in idx])\n",
        "    new_non_cs_y = torch.stack([y for i, y in enumerate(y) if i not in idx])\n",
        "    non_cs = torchdata.TensorDataset(new_non_cs_x, new_non_cs_y)\n",
        "\n",
        "    new_cs = torchdata.TensorDataset(new_cs_x, new_cs_y)\n",
        "    new_task_ids = torch.full((self.size,), task_id)\n",
        "\n",
        "    if self.coreset is None:\n",
        "        self.coreset = new_cs\n",
        "        self.coreset_task_ids = new_task_ids\n",
        "    else:\n",
        "        self.coreset = torchdata.ConcatDataset((self.coreset, new_cs))\n",
        "        self.coreset_task_ids = torch.cat((self.coreset_task_ids, new_task_ids))\n",
        "\n",
        "    return non_cs\n",
        "\n",
        "  def update_distance(self,dists, X, current_id):\n",
        "    current_point = X[current_id]\n",
        "    new_dists = torch.norm(X - current_point, dim=1)\n",
        "    dists = torch.min(dists, new_dists)\n",
        "    return dists\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa2KqMAAM6eR"
      },
      "source": [
        "# VCL Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iBzhPnBpM51B"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MeanFieldGaussianLinear(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, initial_posterior_variance=1e-3):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1e-8\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.ipv = initial_posterior_variance\n",
        "        self.register_buffer('prior_W_means', torch.zeros(out_features, in_features))\n",
        "        self.register_buffer('prior_W_log_vars', torch.zeros(out_features, in_features))\n",
        "        self.register_buffer('prior_b_means', torch.zeros(out_features))\n",
        "        self.register_buffer('prior_b_log_vars', torch.zeros(out_features))\n",
        "\n",
        "        self.posterior_W_means = torch.nn.Parameter(torch.empty_like(self._buffers['prior_W_means'], requires_grad=True))\n",
        "        self.posterior_b_means = torch.nn.Parameter(torch.empty_like(self._buffers['prior_b_means'], requires_grad=True))\n",
        "        self.posterior_W_log_vars = torch.nn.Parameter(torch.empty_like(self._buffers['prior_W_log_vars'], requires_grad=True))\n",
        "        self.posterior_b_log_vars = torch.nn.Parameter(torch.empty_like(self._buffers['prior_b_log_vars'], requires_grad=True))\n",
        "\n",
        "        self._initialize_posteriors()\n",
        "\n",
        "    def forward(self, x, sample_parameters=True):\n",
        "        if sample_parameters:\n",
        "            w, b = self._sample_parameters()\n",
        "            return F.linear(x, w, b)\n",
        "        else:\n",
        "            return F.linear(x, self.posterior_W_means, self.posterior_b_means)\n",
        "\n",
        "    def reset_for_next_task(self):\n",
        "        self._buffers['prior_W_means'].data.copy_(self.posterior_W_means.data)\n",
        "        self._buffers['prior_W_log_vars'].data.copy_(self.posterior_W_log_vars.data)\n",
        "        self._buffers['prior_b_means'].data.copy_(self.posterior_b_means.data)\n",
        "        self._buffers['prior_b_log_vars'].data.copy_(self.posterior_b_log_vars.data)\n",
        "\n",
        "    def kl_divergence(self) -> torch.Tensor:\n",
        "        prior_means = torch.cat((self._buffers['prior_W_means'].view(-1),\n",
        "                                self._buffers['prior_b_means'].view(-1)))\n",
        "        prior_log_vars = torch.cat((self._buffers['prior_W_log_vars'].view(-1),\n",
        "                                    self._buffers['prior_b_log_vars'].view(-1)))\n",
        "        prior_vars = torch.exp(prior_log_vars)\n",
        "\n",
        "        posterior_means = torch.cat((self.posterior_W_means.view(-1),\n",
        "                                    self.posterior_b_means.view(-1)))\n",
        "        posterior_log_vars = torch.cat((self.posterior_W_log_vars.view(-1),\n",
        "                                        self.posterior_b_log_vars.view(-1)))\n",
        "        posterior_vars = torch.exp(posterior_log_vars)\n",
        "\n",
        "        kl_elementwise = (posterior_vars / (prior_vars + self.epsilon) +\n",
        "                          ((prior_means - posterior_means) ** 2) / (prior_vars + self.epsilon) -\n",
        "                          1 + prior_log_vars - posterior_log_vars)\n",
        "\n",
        "        return 0.5 * kl_elementwise.sum()\n",
        "\n",
        "    def count_parameters(self):\n",
        "      return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def get_statistics(self) -> dict:\n",
        "        statistics = {\n",
        "            'total_params': self.count_parameters(),\n",
        "            'average_w_mean': torch.mean(self.posterior_W_means),\n",
        "            'average_b_mean': torch.mean(self.posterior_b_means),\n",
        "            'average_w_var': torch.mean(torch.exp(self.posterior_W_log_vars)),\n",
        "            'average_b_var': torch.mean(torch.exp(self.posterior_b_log_vars))\n",
        "        }\n",
        "\n",
        "        return statistics\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )\n",
        "\n",
        "    def _sample_parameters(self):\n",
        "        w_epsilons = torch.randn_like(self.posterior_W_means)\n",
        "        b_epsilons = torch.randn_like(self.posterior_b_means)\n",
        "\n",
        "        w_std_dev = torch.exp(0.5 * self.posterior_W_log_vars)\n",
        "        b_std_dev = torch.exp(0.5 * self.posterior_b_log_vars)\n",
        "\n",
        "        w = self.posterior_W_means + w_epsilons * w_std_dev\n",
        "        b = self.posterior_b_means + b_epsilons * b_std_dev\n",
        "\n",
        "        return w, b\n",
        "\n",
        "    def _initialize_posteriors(self):\n",
        "        torch.nn.init.normal_(self.posterior_W_means, mean=0, std=0.1)\n",
        "        torch.nn.init.uniform_(self.posterior_b_means, -0.1, 0.1)\n",
        "        torch.nn.init.constant_(self.posterior_W_log_vars, math.log(self.ipv))\n",
        "        torch.nn.init.constant_(self.posterior_b_log_vars, math.log(self.ipv))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDtCQ1hWiqxP"
      },
      "source": [
        "# VCL Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pWBqFohUiuSW"
      },
      "outputs": [],
      "source": [
        "class DiscriminativeVCL(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, x_dim, h_dim, y_dim, n_heads=1, shared_h_dims=(100, 100),\n",
        "                 initial_posterior_variance=1e-6, mc_sampling_n=10, device='cuda'):\n",
        "        super().__init__()\n",
        "        if n_heads < 1:\n",
        "            raise ValueError('Network requires at least one head.')\n",
        "\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.y_dim = y_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.ipv = initial_posterior_variance\n",
        "        self.mc_sampling_n = mc_sampling_n\n",
        "        self.device = device\n",
        "\n",
        "        shared_dims = [x_dim] + list(shared_h_dims) + [h_dim]\n",
        "\n",
        "        self.shared_layers = nn.ModuleList([\n",
        "            MeanFieldGaussianLinear(shared_dims[i], shared_dims[i + 1], self.ipv) for i in\n",
        "            range(len(shared_dims) - 1)\n",
        "        ])\n",
        "        self.heads = nn.ModuleList([\n",
        "            MeanFieldGaussianLinear(self.h_dim, self.y_dim, self.ipv) for _ in range(n_heads)\n",
        "        ])\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x, head_idx, sample_parameters=True):\n",
        "        y_out = torch.zeros(size=(x.size()[0], self.y_dim)).to(self.device)\n",
        "\n",
        "        for _ in range(self.mc_sampling_n if sample_parameters else 1):\n",
        "            h = x\n",
        "            for layer in self.shared_layers:\n",
        "                h = F.relu(layer(h, sample_parameters=sample_parameters))\n",
        "\n",
        "            h = self.heads[head_idx](h, sample_parameters=sample_parameters)\n",
        "            h = self.softmax(h)\n",
        "\n",
        "            y_out.add_(h)\n",
        "\n",
        "        y_out.div_(self.mc_sampling_n)\n",
        "\n",
        "        return y_out\n",
        "\n",
        "    def vcl_loss(self, x, y, head_idx, task_size):\n",
        "        return self._kl_divergence(head_idx) / task_size + torch.nn.NLLLoss()(self(x, head_idx), y.long())\n",
        "\n",
        "    def point_estimate_loss(self, x, y, head_idx):\n",
        "        return torch.nn.NLLLoss()(self(x, head_idx, sample_parameters=False), y)\n",
        "\n",
        "    def prediction(self, x, task):\n",
        "        return torch.argmax(self(x, task), dim=1)\n",
        "\n",
        "    def reset_for_new_task(self, head_idx):\n",
        "        for layer in self.shared_layers:\n",
        "            layer.reset_for_next_task()\n",
        "\n",
        "        self.heads[head_idx].reset_for_next_task()\n",
        "\n",
        "    def get_statistics(self):\n",
        "        layer_statistics = []\n",
        "        model_statistics = {\n",
        "            'total_params' : 0,\n",
        "            'average_w_mean': 0,\n",
        "            'average_b_mean': 0,\n",
        "            'average_w_var': 0,\n",
        "            'average_b_var': 0\n",
        "        }\n",
        "\n",
        "        for layer in self.shared_layers:\n",
        "            stats = layer.get_statistics()\n",
        "            layer_statistics.append(stats)\n",
        "            layer_n_params = stats['total_params']\n",
        "            model_statistics['total_params'] += layer_n_params\n",
        "            model_statistics['average_w_mean'] += stats['average_w_mean']*layer_n_params\n",
        "            model_statistics['average_b_mean'] += stats['average_b_mean']*layer_n_params\n",
        "            model_statistics['average_w_var'] += stats['average_w_var']*layer_n_params\n",
        "            model_statistics['average_b_var'] += stats['average_b_var']*layer_n_params\n",
        "\n",
        "        for head in self.heads:\n",
        "            stats = head.get_statistics()\n",
        "            layer_statistics.append(stats)\n",
        "            layer_n_params = stats['total_params']\n",
        "            model_statistics['total_params'] += layer_n_params\n",
        "            model_statistics['average_w_mean'] += stats['average_w_mean']*layer_n_params\n",
        "            model_statistics['average_b_mean'] += stats['average_b_mean']*layer_n_params\n",
        "            model_statistics['average_w_var'] += stats['average_w_var']*layer_n_params\n",
        "            model_statistics['average_b_var'] += stats['average_b_var']*layer_n_params\n",
        "\n",
        "        model_statistics['average_w_mean'] /= model_statistics['total_params']\n",
        "        model_statistics['average_b_mean'] /= model_statistics['total_params']\n",
        "        model_statistics['average_w_var'] /= model_statistics['total_params']\n",
        "        model_statistics['average_b_var'] /= model_statistics['total_params']\n",
        "\n",
        "        return layer_statistics, model_statistics\n",
        "\n",
        "    def _kl_divergence(self, head_idx) -> torch.Tensor:\n",
        "        kl_divergence = torch.zeros(1, requires_grad=False).to(self.device)\n",
        "\n",
        "        for layer in self.shared_layers:\n",
        "            kl_divergence = torch.add(kl_divergence, layer.kl_divergence())\n",
        "\n",
        "        kl_divergence = torch.add(kl_divergence, self.heads[head_idx].kl_divergence())\n",
        "        return kl_divergence\n",
        "\n",
        "    def _mean_posterior_variance(self):\n",
        "        ((_, posterior_w_log_vars), (_, posterior_b_log_vars)) = self.posterior\n",
        "        posterior_log_vars = torch.cat([torch.reshape(t, (-1,)) for t in posterior_w_log_vars] + posterior_b_log_vars)\n",
        "        posterior_vars     = torch.exp(posterior_log_vars)\n",
        "        return torch.mean(posterior_vars).item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKspXYYA0k4S"
      },
      "source": [
        "# Discriminative Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rFiHpcB0lQi",
        "outputId": "0287bc96-0f53-48f5-9b59-6aa841aea2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device cuda:0\n"
          ]
        }
      ],
      "source": [
        "MNIST_FLATTENED_DIM = 28 * 28\n",
        "LR = 0.001\n",
        "INITIAL_POSTERIOR_VAR = 1e-3\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "print(\"Running on device\", device)\n",
        "\n",
        "def permuted_mnist(coresetAlgo,coreset_size=20):\n",
        "    N_CLASSES = 10\n",
        "    LAYER_WIDTH = 100\n",
        "    N_HIDDEN_LAYERS = 2\n",
        "    N_TASKS = 10\n",
        "    MULTIHEADED = False\n",
        "    CORESET_SIZE = coreset_size\n",
        "    EPOCHS = 100\n",
        "    BATCH_SIZE = 256\n",
        "    TRAIN_FULL_CORESET = True\n",
        "    coresetDict = {\"Random\":RandomCoreset, \"K-Center\":kCenterCoreset}\n",
        "\n",
        "    transforms = [Compose([Flatten(), Scale(), Permute(torch.randperm(MNIST_FLATTENED_DIM))]) for _ in range(N_TASKS)]\n",
        "\n",
        "\n",
        "    model = DiscriminativeVCL(\n",
        "        x_dim=MNIST_FLATTENED_DIM, y_dim=N_CLASSES,\n",
        "        h_dim=LAYER_WIDTH, shared_h_dims = tuple(LAYER_WIDTH for _ in range(N_HIDDEN_LAYERS)),\n",
        "        n_heads=(N_TASKS if MULTIHEADED else 1),\n",
        "        initial_posterior_variance=INITIAL_POSTERIOR_VAR\n",
        "    ).to(device)\n",
        "\n",
        "    coreset = coresetDict[coresetAlgo](size=CORESET_SIZE)\n",
        "\n",
        "    mnist_train = ConcatDataset(\n",
        "        [MNIST(root=\"data\", train=True, download=True, transform=t) for t in transforms]\n",
        "    )\n",
        "    mnist_train = Subset(mnist_train,torch.randperm(len(mnist_train))[:len(mnist_train)//20])\n",
        "    task_size = len(mnist_train) // N_TASKS\n",
        "    train_task_ids = torch.cat(\n",
        "        [torch.full((task_size,), id) for id in range(N_TASKS)]\n",
        "    )\n",
        "\n",
        "    mnist_test = ConcatDataset(\n",
        "        [MNIST(root=\"data\", train=False, download=True, transform=t) for t in transforms]\n",
        "    )\n",
        "    mnist_test = Subset(mnist_test,torch.randperm(len(mnist_test))[:len(mnist_test)//20])\n",
        "    task_size = len(mnist_test) // N_TASKS\n",
        "    test_task_ids = torch.cat(\n",
        "        [torch.full((task_size,), id) for id in range(N_TASKS)]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    summary_logdir = os.path.join(\"logs\", \"disc_p_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n",
        "    writer = SummaryWriter(summary_logdir)\n",
        "\n",
        "    run_point_estimate_initialisation(model=model, data=mnist_train,\n",
        "                                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                                      device=device, lr=LR,\n",
        "                                      multiheaded=MULTIHEADED,\n",
        "                                      task_ids=train_task_ids)\n",
        "    accuracies_list = []\n",
        "    for task in range(N_TASKS):\n",
        "        accuracies = run_task(\n",
        "            model=model, train_data=mnist_train, train_task_ids=train_task_ids,\n",
        "            test_data=mnist_test, test_task_ids=test_task_ids, task_idx=task,\n",
        "            coreset=coreset, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "            device=device, lr=LR,num_tasks=10, save_as=\"disc_p_mnist\",\n",
        "            multiheaded=MULTIHEADED, train_full_coreset=TRAIN_FULL_CORESET,\n",
        "            summary_writer=writer\n",
        "        )\n",
        "        accuracies_list.append(accuracies)\n",
        "    accuracies_stack = np.stack(accuracies_list)\n",
        "    avgAcc = np.nanmean(accuracies_stack, axis=0)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    return avgAcc,accuracies_list\n",
        "\n",
        "\n",
        "def split_mnist(coresetAlgo,coreset_size=4):\n",
        "\n",
        "    N_CLASSES = 2\n",
        "    LAYER_WIDTH = 256\n",
        "    N_HIDDEN_LAYERS = 2\n",
        "    N_TASKS = 5\n",
        "    MULTIHEADED = True\n",
        "    CORESET_SIZE = coreset_size\n",
        "    EPOCHS = 100\n",
        "    BATCH_SIZE = 256\n",
        "    TRAIN_FULL_CORESET = True\n",
        "    coresetDict = {\"Random\":RandomCoreset, \"K-Center\":kCenterCoreset}\n",
        "\n",
        "    transform = Compose([Flatten(), Scale()])\n",
        "\n",
        "    mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "    mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "    mnist_train = Subset(mnist_train,torch.randperm(len(mnist_train))[:len(mnist_train)])\n",
        "    task_size = len(mnist_train) // N_TASKS\n",
        "    train_task_ids = torch.cat(\n",
        "        [torch.full((task_size,), id) for id in range(N_TASKS)]\n",
        "    )\n",
        "\n",
        "    mnist_test = Subset(mnist_test,torch.randperm(len(mnist_test))[:len(mnist_test)])\n",
        "    task_size = len(mnist_test) // N_TASKS\n",
        "    test_task_ids = torch.cat(\n",
        "        [torch.full((task_size,), id) for id in range(N_TASKS)]\n",
        "    )\n",
        "    model = DiscriminativeVCL(\n",
        "        x_dim=MNIST_FLATTENED_DIM, y_dim=N_CLASSES,\n",
        "        h_dim=LAYER_WIDTH, shared_h_dims = tuple(LAYER_WIDTH for _ in range(N_HIDDEN_LAYERS)),\n",
        "        n_heads=(N_TASKS if MULTIHEADED else 1),\n",
        "        initial_posterior_variance=INITIAL_POSTERIOR_VAR\n",
        "    ).to(device)\n",
        "\n",
        "    coreset = coresetDict[coresetAlgo](size=CORESET_SIZE)\n",
        "    label_to_task_mapping = {\n",
        "        0: 0, 1: 0,\n",
        "        2: 1, 3: 1,\n",
        "        4: 2, 5: 2,\n",
        "        6: 3, 7: 3,\n",
        "        8: 4, 9: 4,\n",
        "    }\n",
        "\n",
        "    if isinstance(mnist_train[0][1], int):\n",
        "        train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_train])\n",
        "        test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_test])\n",
        "    elif isinstance(mnist_train[0][1], torch.Tensor):\n",
        "        train_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_train])\n",
        "        test_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_test])\n",
        "\n",
        "    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n",
        "    writer = SummaryWriter(summary_logdir)\n",
        "\n",
        "    binarize_y = lambda y, task: (y == (2 * task + 1)).long()\n",
        "\n",
        "    run_point_estimate_initialisation(model=model, data=mnist_train,\n",
        "                                      epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                                      device=device, multiheaded=MULTIHEADED,\n",
        "                                      lr=LR, task_ids=train_task_ids,\n",
        "                                      y_transform=binarize_y)\n",
        "\n",
        "    accuracies_list = []\n",
        "    for task_idx in range(N_TASKS):\n",
        "        accuracies = run_task(\n",
        "            model=model, train_data=mnist_train, train_task_ids=train_task_ids,\n",
        "            test_data=mnist_test, test_task_ids=test_task_ids, coreset=coreset,\n",
        "            task_idx=task_idx, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR,num_tasks=5,\n",
        "            save_as=\"disc_s_mnist\", device=device, multiheaded=MULTIHEADED,\n",
        "            y_transform=binarize_y, train_full_coreset=TRAIN_FULL_CORESET,\n",
        "            summary_writer=writer\n",
        "        )\n",
        "        accuracies_list.append(accuracies)\n",
        "    accuracies_stack = np.stack(accuracies_list)\n",
        "    avgAcc = np.nanmean(accuracies_stack, axis=0)\n",
        "\n",
        "\n",
        "    writer.close()\n",
        "    return avgAcc,accuracies_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf_73029AjAN"
      },
      "source": [
        "# Conv Layer and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "TKFJd2qGupFr"
      },
      "outputs": [],
      "source": [
        "class MeanFieldGaussianConv2d(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, initial_posterior_variance=1e-3):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1e-8\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.ipv = initial_posterior_variance\n",
        "        self.kernel_size = kernel_size\n",
        "        self.groups = 1\n",
        "\n",
        "        self.register_buffer('prior_W_means', torch.zeros(out_channels, in_channels//self.groups, kernel_size,kernel_size))\n",
        "        self.register_buffer('prior_W_log_vars', torch.zeros(out_channels, in_channels//self.groups, kernel_size,kernel_size))\n",
        "        self.register_buffer('prior_b_means', torch.zeros(out_channels))\n",
        "        self.register_buffer('prior_b_log_vars', torch.zeros(out_channels))\n",
        "\n",
        "        self.posterior_W_means = torch.nn.Parameter(torch.empty_like(self._buffers['prior_W_means'], requires_grad=True))\n",
        "        self.posterior_b_means = torch.nn.Parameter(torch.empty_like(self._buffers['prior_b_means'], requires_grad=True))\n",
        "        self.posterior_W_log_vars = torch.nn.Parameter(torch.empty_like(self._buffers['prior_W_log_vars'], requires_grad=True))\n",
        "        self.posterior_b_log_vars = torch.nn.Parameter(torch.empty_like(self._buffers['prior_b_log_vars'], requires_grad=True))\n",
        "\n",
        "        self._initialize_posteriors()\n",
        "\n",
        "    def forward(self, x, sample_parameters=True):\n",
        "        if sample_parameters:\n",
        "            w, b = self._sample_parameters()\n",
        "            return F.conv2d(input=x, weight=w, bias=b,groups=self.groups,padding=1)\n",
        "        else:\n",
        "            return F.conv2d(input=x, weight=self.posterior_W_means, bias=self.posterior_b_means,groups=self.groups,padding=1)\n",
        "\n",
        "    def reset_for_next_task(self):\n",
        "        self._buffers['prior_W_means'].data.copy_(self.posterior_W_means.data)\n",
        "        self._buffers['prior_W_log_vars'].data.copy_(self.posterior_W_log_vars.data)\n",
        "        self._buffers['prior_b_means'].data.copy_(self.posterior_b_means.data)\n",
        "        self._buffers['prior_b_log_vars'].data.copy_(self.posterior_b_log_vars.data)\n",
        "\n",
        "    def kl_divergence(self) -> torch.Tensor:\n",
        "        prior_means = torch.cat((self._buffers['prior_W_means'].view(-1),\n",
        "                                self._buffers['prior_b_means'].view(-1)))\n",
        "        prior_log_vars = torch.cat((self._buffers['prior_W_log_vars'].view(-1),\n",
        "                                    self._buffers['prior_b_log_vars'].view(-1)))\n",
        "        prior_vars = torch.exp(prior_log_vars)\n",
        "\n",
        "        posterior_means = torch.cat((self.posterior_W_means.view(-1),\n",
        "                                    self.posterior_b_means.view(-1)))\n",
        "        posterior_log_vars = torch.cat((self.posterior_W_log_vars.view(-1),\n",
        "                                        self.posterior_b_log_vars.view(-1)))\n",
        "        posterior_vars = torch.exp(posterior_log_vars)\n",
        "\n",
        "        kl_elementwise = (posterior_vars / (prior_vars + self.epsilon) +\n",
        "                          ((prior_means - posterior_means) ** 2) / (prior_vars + self.epsilon) -\n",
        "                          1 + prior_log_vars - posterior_log_vars)\n",
        "\n",
        "        return 0.5 * kl_elementwise.sum()\n",
        "\n",
        "    def count_parameters(self):\n",
        "      return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def get_statistics(self) -> dict:\n",
        "        statistics = {\n",
        "            'total_params': self.count_parameters(),\n",
        "            'average_w_mean': torch.mean(self.posterior_W_means),\n",
        "            'average_b_mean': torch.mean(self.posterior_b_means),\n",
        "            'average_w_var': torch.mean(torch.exp(self.posterior_W_log_vars)),\n",
        "            'average_b_var': torch.mean(torch.exp(self.posterior_b_log_vars))\n",
        "        }\n",
        "\n",
        "        return statistics\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_channels, self.out_channels, self.bias is not None\n",
        "        )\n",
        "\n",
        "    def _sample_parameters(self):\n",
        "        w_epsilons = torch.randn_like(self.posterior_W_means)\n",
        "        b_epsilons = torch.randn_like(self.posterior_b_means)\n",
        "\n",
        "        w_std_dev = torch.exp(0.5 * self.posterior_W_log_vars)\n",
        "        b_std_dev = torch.exp(0.5 * self.posterior_b_log_vars)\n",
        "\n",
        "        w = self.posterior_W_means + w_epsilons * w_std_dev\n",
        "        b = self.posterior_b_means + b_epsilons * b_std_dev\n",
        "\n",
        "        return w, b\n",
        "\n",
        "    def _initialize_posteriors(self):\n",
        "        torch.nn.init.normal_(self.posterior_W_means, mean=0, std=0.1)\n",
        "        torch.nn.init.uniform_(self.posterior_b_means, -0.1, 0.1)\n",
        "        torch.nn.init.constant_(self.posterior_W_log_vars, math.log(self.ipv))\n",
        "        torch.nn.init.constant_(self.posterior_b_log_vars, math.log(self.ipv))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "9Q_UlfpVAgc7"
      },
      "outputs": [],
      "source": [
        "class ConvVCL(nn.Module):\n",
        "    def __init__(self, x_dim,x_channels, y_dim, n_heads=1, shared_h_channels=(32,64,128),kernel_size=3,\n",
        "                 initial_posterior_variance=1e-6, mc_sampling_n=10, device='cuda'):\n",
        "        super().__init__()\n",
        "        if n_heads < 1:\n",
        "            raise ValueError('Network requires at least one head.')\n",
        "\n",
        "        self.x_dim = x_dim\n",
        "        self.x_channels = x_channels\n",
        "        self.y_dim = y_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.kernel_size = kernel_size\n",
        "        self.ipv = initial_posterior_variance\n",
        "        self.mc_sampling_n = mc_sampling_n\n",
        "        self.device = device\n",
        "\n",
        "        shared_channels = [x_channels]+list(shared_h_channels)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.shared_layers = nn.ModuleList([\n",
        "            MeanFieldGaussianConv2d(shared_channels[i], shared_channels[i + 1], self.kernel_size, initial_posterior_variance = self.ipv) for i in\n",
        "            range(len(shared_channels) - 1)\n",
        "        ])\n",
        "        self.output_flat_length = self.get_output_shape()\n",
        "        self.heads = [\n",
        "            [MeanFieldGaussianLinear(self.output_flat_length, 256, self.ipv).to(\"cuda\"),MeanFieldGaussianLinear(256, self.y_dim, self.ipv).to(\"cuda\")] for _ in range(n_heads)\n",
        "        ]\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    def get_output_shape(self):\n",
        "        x = torch.rand((1,self.x_channels,self.x_dim,self.x_dim))\n",
        "        for layer in self.shared_layers:\n",
        "            x = self.pool(F.relu(layer(x, sample_parameters=False)))\n",
        "\n",
        "        return math.prod(x.shape)\n",
        "    def forward(self, x, head_idx, sample_parameters=True):\n",
        "        y_out = torch.zeros(size=(x.size()[0], self.y_dim)).to(self.device)\n",
        "\n",
        "        for _ in range(self.mc_sampling_n if sample_parameters else 1):\n",
        "            h = x\n",
        "            for layer in self.shared_layers:\n",
        "                h = self.pool(F.relu(layer(h, sample_parameters=sample_parameters)))\n",
        "            h = h.view(-1,self.output_flat_length)\n",
        "            h = F.relu(self.heads[head_idx][0](h, sample_parameters=sample_parameters))\n",
        "            h = self.heads[head_idx][1](h,sample_parameters=sample_parameters)\n",
        "            h = self.softmax(h)\n",
        "\n",
        "            y_out.add_(h)\n",
        "\n",
        "        y_out.div_(self.mc_sampling_n)\n",
        "\n",
        "        return y_out\n",
        "\n",
        "    def vcl_loss(self, x, y, head_idx, task_size):\n",
        "        return self._kl_divergence(head_idx) / task_size + torch.nn.NLLLoss()(self(x, head_idx), y.long())\n",
        "\n",
        "    def point_estimate_loss(self, x, y, head_idx):\n",
        "        return torch.nn.NLLLoss()(self(x, head_idx, sample_parameters=False), y)\n",
        "\n",
        "    def prediction(self, x, task):\n",
        "        return torch.argmax(self(x, task), dim=1)\n",
        "\n",
        "    def reset_for_new_task(self, head_idx):\n",
        "        for layer in self.shared_layers:\n",
        "            layer.reset_for_next_task()\n",
        "\n",
        "        self.heads[head_idx][0].reset_for_next_task()\n",
        "        self.heads[head_idx][1].reset_for_next_task()\n",
        "\n",
        "\n",
        "    def get_statistics(self):\n",
        "        layer_statistics = []\n",
        "        model_statistics = {\n",
        "            'total_params' : 0,\n",
        "            'average_w_mean': 0,\n",
        "            'average_b_mean': 0,\n",
        "            'average_w_var': 0,\n",
        "            'average_b_var': 0\n",
        "        }\n",
        "\n",
        "        for layer in self.shared_layers:\n",
        "            stats = layer.get_statistics()\n",
        "            layer_statistics.append(stats)\n",
        "            layer_n_params = stats['total_params']\n",
        "            model_statistics['total_params'] += layer_n_params\n",
        "            model_statistics['average_w_mean'] += stats['average_w_mean']*layer_n_params\n",
        "            model_statistics['average_b_mean'] += stats['average_b_mean']*layer_n_params\n",
        "            model_statistics['average_w_var'] += stats['average_w_var']*layer_n_params\n",
        "            model_statistics['average_b_var'] += stats['average_b_var']*layer_n_params\n",
        "\n",
        "        for headL in self.heads:\n",
        "          for head in headL:\n",
        "            stats = head.get_statistics()\n",
        "            layer_statistics.append(stats)\n",
        "            layer_n_params = stats['total_params']\n",
        "            model_statistics['total_params'] += layer_n_params\n",
        "            model_statistics['average_w_mean'] += stats['average_w_mean']*layer_n_params\n",
        "            model_statistics['average_b_mean'] += stats['average_b_mean']*layer_n_params\n",
        "            model_statistics['average_w_var'] += stats['average_w_var']*layer_n_params\n",
        "            model_statistics['average_b_var'] += stats['average_b_var']*layer_n_params\n",
        "\n",
        "        model_statistics['average_w_mean'] /= model_statistics['total_params']\n",
        "        model_statistics['average_b_mean'] /= model_statistics['total_params']\n",
        "        model_statistics['average_w_var'] /= model_statistics['total_params']\n",
        "        model_statistics['average_b_var'] /= model_statistics['total_params']\n",
        "\n",
        "        return layer_statistics, model_statistics\n",
        "\n",
        "    def _kl_divergence(self, head_idx) -> torch.Tensor:\n",
        "        kl_divergence = torch.zeros(1, requires_grad=False).to(self.device)\n",
        "        for layer in self.shared_layers:\n",
        "            kl_divergence = torch.add(kl_divergence, layer.kl_divergence())\n",
        "\n",
        "        kl_divergence = torch.add(kl_divergence, self.heads[head_idx][0].kl_divergence())\n",
        "        kl_divergence = torch.add(kl_divergence, self.heads[head_idx][1].kl_divergence())\n",
        "        return kl_divergence\n",
        "\n",
        "    def _mean_posterior_variance(self):\n",
        "        ((_, posterior_w_log_vars), (_, posterior_b_log_vars)) = self.posterior\n",
        "        posterior_log_vars = torch.cat([torch.reshape(t, (-1,)) for t in posterior_w_log_vars] + posterior_b_log_vars)\n",
        "        posterior_vars     = torch.exp(posterior_log_vars)\n",
        "        return torch.mean(posterior_vars).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CVCL tests MNIST and CIFAR10"
      ],
      "metadata": {
        "id": "vfoWUOr3uzwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_J_hNzLpZn3d"
      },
      "outputs": [],
      "source": [
        "def split_mnist_CVCL(coresetAlgo,coreset_size=4):\n",
        "    N_CLASSES = 2\n",
        "    N_TASKS = 5\n",
        "    MULTIHEADED = True\n",
        "    CORESET_SIZE = coreset_size\n",
        "    EPOCHS = 100\n",
        "    BATCH_SIZE = 64\n",
        "    TRAIN_FULL_CORESET = True\n",
        "    coresetDict = {\"Random\":RandomCoreset, \"K-Center\":kCenterCoreset}\n",
        "\n",
        "    transform = Compose([ToTensor(),Scale()])\n",
        "\n",
        "    mnist_train = MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "    mnist_test = MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "    mnist_train = Subset(mnist_train,torch.randperm(len(mnist_train))[:len(mnist_train)])\n",
        "    task_size = len(mnist_train) // N_TASKS\n",
        "    train_task_ids = torch.cat(\n",
        "        [torch.full((task_size,), id) for id in range(N_TASKS)]\n",
        "    )\n",
        "\n",
        "    mnist_test = Subset(mnist_test,torch.randperm(len(mnist_test))[:len(mnist_test)])\n",
        "    task_size = len(mnist_test) // N_TASKS\n",
        "    test_task_ids = torch.cat(\n",
        "        [torch.full((task_size,), id) for id in range(N_TASKS)]\n",
        "    )\n",
        "    model = ConvVCL(x_channels=1,\n",
        "        x_dim=28, y_dim=N_CLASSES,\n",
        "        n_heads=(N_TASKS if MULTIHEADED else 1),\n",
        "        initial_posterior_variance=INITIAL_POSTERIOR_VAR\n",
        "    ).to(device)\n",
        "\n",
        "    coreset = coresetDict[coresetAlgo](size=CORESET_SIZE)\n",
        "    label_to_task_mapping = {\n",
        "        0: 0, 1: 0,\n",
        "        2: 1, 3: 1,\n",
        "        4: 2, 5: 2,\n",
        "        6: 3, 7: 3,\n",
        "        8: 4, 9: 4,\n",
        "    }\n",
        "\n",
        "    if isinstance(mnist_train[0][1], int):\n",
        "        train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_train])\n",
        "        test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in mnist_test])\n",
        "    elif isinstance(mnist_train[0][1], torch.Tensor):\n",
        "        train_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_train])\n",
        "        test_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in mnist_test])\n",
        "\n",
        "    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n",
        "    writer = SummaryWriter(summary_logdir)\n",
        "\n",
        "    binarize_y = lambda y, task: (y == (2 * task + 1)).long()\n",
        "\n",
        "    run_point_estimate_initialisation(model=model, data=mnist_train,\n",
        "                                      epochs=2, batch_size=BATCH_SIZE,\n",
        "                                      device=device, multiheaded=MULTIHEADED,\n",
        "                                      lr=LR, task_ids=train_task_ids,\n",
        "                                      y_transform=binarize_y)\n",
        "\n",
        "    accuracies_list = []\n",
        "    for task_idx in range(N_TASKS):\n",
        "        accuracies = run_task(\n",
        "            model=model, train_data=mnist_train, train_task_ids=train_task_ids,\n",
        "            test_data=mnist_test, test_task_ids=test_task_ids, coreset=coreset,\n",
        "            task_idx=task_idx, epochs=2, batch_size=BATCH_SIZE, lr=LR,num_tasks=5,\n",
        "            save_as=\"disc_s_mnist\", device=device, multiheaded=MULTIHEADED,\n",
        "            y_transform=binarize_y, train_full_coreset=TRAIN_FULL_CORESET,\n",
        "            summary_writer=writer\n",
        "        )\n",
        "        accuracies_list.append(accuracies)\n",
        "    accuracies_stack = np.stack(accuracies_list)\n",
        "    avgAcc = np.nanmean(accuracies_stack, axis=0)\n",
        "\n",
        "\n",
        "    writer.close()\n",
        "    return avgAcc,accuracies_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_cifar_CVCL(coresetAlgo,coreset_size=4):\n",
        "    N_CLASSES = 2\n",
        "    N_TASKS = 5\n",
        "    MULTIHEADED = True\n",
        "    CORESET_SIZE = coreset_size\n",
        "    EPOCHS = 100\n",
        "    BATCH_SIZE = 64\n",
        "    TRAIN_FULL_CORESET = True\n",
        "    coresetDict = {\"Random\":RandomCoreset, \"K-Center\":kCenterCoreset}\n",
        "\n",
        "    transform = Compose([ToTensor(),Scale()])\n",
        "\n",
        "    cifar_train = CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
        "    cifar_test = CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "    cifar_train = Subset(cifar_train,torch.randperm(len(cifar_train))[:len(cifar_train)])\n",
        "    task_size = len(cifar_train) // N_TASKS\n",
        "    train_task_ids = torch.cat(\n",
        "        [torch.full((task_size,), id) for id in range(N_TASKS)]\n",
        "    )\n",
        "\n",
        "    cifar_test = Subset(cifar_test,torch.randperm(len(cifar_test))[:len(cifar_test)])\n",
        "    task_size = len(cifar_test) // N_TASKS\n",
        "    test_task_ids = torch.cat(\n",
        "        [torch.full((task_size,), id) for id in range(N_TASKS)]\n",
        "    )\n",
        "    model = ConvVCL(x_channels=3,\n",
        "        x_dim=32, y_dim=N_CLASSES,\n",
        "        n_heads=(N_TASKS if MULTIHEADED else 1),\n",
        "        initial_posterior_variance=INITIAL_POSTERIOR_VAR\n",
        "    ).to(device)\n",
        "\n",
        "    coreset = coresetDict[coresetAlgo](size=CORESET_SIZE)\n",
        "    label_to_task_mapping = {\n",
        "        0: 0, 1: 0,\n",
        "        2: 1, 3: 1,\n",
        "        4: 2, 5: 2,\n",
        "        6: 3, 7: 3,\n",
        "        8: 4, 9: 4,\n",
        "    }\n",
        "\n",
        "    if isinstance(cifar_train[0][1], int):\n",
        "        train_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in cifar_train])\n",
        "        test_task_ids = torch.Tensor([label_to_task_mapping[y] for _, y in cifar_test])\n",
        "    elif isinstance(cifar_train[0][1], torch.Tensor):\n",
        "        train_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in cifar_train])\n",
        "        test_task_ids = torch.Tensor([label_to_task_mapping[y.item()] for _, y in cifar_test])\n",
        "\n",
        "    summary_logdir = os.path.join(\"logs\", \"disc_s_mnist\", datetime.now().strftime('%b%d_%H-%M-%S'))\n",
        "    writer = SummaryWriter(summary_logdir)\n",
        "\n",
        "    binarize_y = lambda y, task: (y == (2 * task + 1)).long()\n",
        "\n",
        "    run_point_estimate_initialisation(model=model, data=cifar_train,\n",
        "                                      epochs=1, batch_size=BATCH_SIZE,\n",
        "                                      device=device, multiheaded=MULTIHEADED,\n",
        "                                      lr=LR, task_ids=train_task_ids,\n",
        "                                      y_transform=binarize_y)\n",
        "\n",
        "    accuracies_list = []\n",
        "    for task_idx in range(N_TASKS):\n",
        "        accuracies = run_task(\n",
        "            model=model, train_data=cifar_train, train_task_ids=train_task_ids,\n",
        "            test_data=cifar_test, test_task_ids=test_task_ids, coreset=coreset,\n",
        "            task_idx=task_idx, epochs=1, batch_size=BATCH_SIZE, lr=LR,num_tasks=5,\n",
        "            save_as=\"disc_s_mnist\", device=device, multiheaded=MULTIHEADED,\n",
        "            y_transform=binarize_y, train_full_coreset=TRAIN_FULL_CORESET,\n",
        "            summary_writer=writer\n",
        "        )\n",
        "        accuracies_list.append(accuracies)\n",
        "    accuracies_stack = np.stack(accuracies_list)\n",
        "    avgAcc = np.nanmean(accuracies_stack, axis=0)\n",
        "\n",
        "\n",
        "    writer.close()\n",
        "    return avgAcc,accuracies_list"
      ],
      "metadata": {
        "id": "PJr4fyOTRVO4"
      },
      "execution_count": 51,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}